{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \n",
    "    source = '/home/aman/Learnings/Datasets/nlp/'\n",
    "    filename = source + filename\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuations\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    source = '/home/aman/Learnings/Datasets/nlp/'\n",
    "    filename = source+filename\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesstext(inFileName, outFileName):\n",
    "        \n",
    "    doc = load_doc(inFileName)\n",
    "    print(doc[:200])\n",
    "    \n",
    "    tokens = clean_doc(doc)\n",
    "    print(tokens[:200])\n",
    "    print(f'Total tokens: {len(tokens)}')\n",
    "    print(f'total unique tokens: {len(set(tokens))}')\n",
    "    \n",
    "    length = 50+1\n",
    "    sequences = list()\n",
    "    for i in range(length, len(tokens)):\n",
    "        # select sequences of tokens\n",
    "        seq = tokens[i-length:i]\n",
    "        # convert into a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "        \n",
    "    print(sequences[:5])    \n",
    "    print(f'Total Sequences: {len(sequences)}')\n",
    "    \n",
    "    save_doc(sequences, outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK I.\n",
      "\n",
      "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
      "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
      "Artemis.); and also because I wanted to see in what\n",
      "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
      "Total tokens: 118684\n",
      "total unique tokens: 7409\n",
      "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was', 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted', 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with', 'went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the', 'down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with the procession']\n",
      "Total Sequences: 118633\n"
     ]
    }
   ],
   "source": [
    "preprocesstext('TheRepublic.txt', 'TheRepublic_Sequences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('TheRepublic_Sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes = vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 50)            370500    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7410)              748410    \n",
      "=================================================================\n",
      "Total params: 1,269,810\n",
      "Trainable params: 1,269,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50 , input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "118633/118633 [==============================] - 97s 821us/step - loss: 2.2032 - acc: 0.4817\n",
      "Epoch 2/300\n",
      "118633/118633 [==============================] - 95s 804us/step - loss: 2.2378 - acc: 0.4766\n",
      "Epoch 3/300\n",
      "118633/118633 [==============================] - 104s 877us/step - loss: 2.2273 - acc: 0.4799\n",
      "Epoch 4/300\n",
      "118633/118633 [==============================] - 96s 806us/step - loss: 2.1862 - acc: 0.4879\n",
      "Epoch 5/300\n",
      "118633/118633 [==============================] - 146s 1ms/step - loss: 2.1515 - acc: 0.4927\n",
      "Epoch 6/300\n",
      "118633/118633 [==============================] - 123s 1ms/step - loss: 2.1338 - acc: 0.4957\n",
      "Epoch 7/300\n",
      "118633/118633 [==============================] - 148s 1ms/step - loss: 2.1207 - acc: 0.5003\n",
      "Epoch 8/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 2.1113 - acc: 0.5001\n",
      "Epoch 9/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 2.0981 - acc: 0.5022\n",
      "Epoch 10/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 2.0815 - acc: 0.5061\n",
      "Epoch 11/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 2.0595 - acc: 0.5102\n",
      "Epoch 12/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 2.0677 - acc: 0.5095\n",
      "Epoch 13/300\n",
      "118633/118633 [==============================] - 126s 1ms/step - loss: 2.1043 - acc: 0.5012\n",
      "Epoch 14/300\n",
      "118633/118633 [==============================] - 96s 808us/step - loss: 2.0117 - acc: 0.5203\n",
      "Epoch 15/300\n",
      "118633/118633 [==============================] - 106s 890us/step - loss: 2.0337 - acc: 0.5143\n",
      "Epoch 16/300\n",
      "118633/118633 [==============================] - 102s 864us/step - loss: 1.9970 - acc: 0.5219\n",
      "Epoch 17/300\n",
      "118633/118633 [==============================] - 100s 844us/step - loss: 1.9834 - acc: 0.5259\n",
      "Epoch 18/300\n",
      "118633/118633 [==============================] - 105s 887us/step - loss: 1.9560 - acc: 0.5309\n",
      "Epoch 19/300\n",
      "118633/118633 [==============================] - 100s 839us/step - loss: 1.9399 - acc: 0.5343\n",
      "Epoch 20/300\n",
      "118633/118633 [==============================] - 103s 866us/step - loss: 1.9343 - acc: 0.5351\n",
      "Epoch 21/300\n",
      "118633/118633 [==============================] - 104s 877us/step - loss: 1.9152 - acc: 0.5380\n",
      "Epoch 22/300\n",
      "118633/118633 [==============================] - 103s 867us/step - loss: 1.9051 - acc: 0.5393\n",
      "Epoch 23/300\n",
      "118633/118633 [==============================] - 103s 870us/step - loss: 1.8928 - acc: 0.5416\n",
      "Epoch 24/300\n",
      "118633/118633 [==============================] - 106s 895us/step - loss: 1.8810 - acc: 0.5434\n",
      "Epoch 25/300\n",
      "118633/118633 [==============================] - 104s 878us/step - loss: 1.8696 - acc: 0.5464\n",
      "Epoch 26/300\n",
      "118633/118633 [==============================] - 104s 875us/step - loss: 1.8513 - acc: 0.5509\n",
      "Epoch 27/300\n",
      "118633/118633 [==============================] - 151s 1ms/step - loss: 1.8483 - acc: 0.5499\n",
      "Epoch 28/300\n",
      "118633/118633 [==============================] - 108s 911us/step - loss: 1.8474 - acc: 0.5522\n",
      "Epoch 29/300\n",
      "118633/118633 [==============================] - 99s 837us/step - loss: 1.8173 - acc: 0.5575\n",
      "Epoch 30/300\n",
      "118633/118633 [==============================] - 102s 861us/step - loss: 1.8294 - acc: 0.5543\n",
      "Epoch 31/300\n",
      "118633/118633 [==============================] - 103s 865us/step - loss: 1.8945 - acc: 0.5456\n",
      "Epoch 32/300\n",
      "118633/118633 [==============================] - 106s 895us/step - loss: 1.9396 - acc: 0.5426\n",
      "Epoch 33/300\n",
      "118633/118633 [==============================] - 106s 891us/step - loss: 1.9175 - acc: 0.5476\n",
      "Epoch 34/300\n",
      "118633/118633 [==============================] - 106s 890us/step - loss: 1.9004 - acc: 0.5468\n",
      "Epoch 35/300\n",
      "118633/118633 [==============================] - 106s 891us/step - loss: 1.8272 - acc: 0.5605\n",
      "Epoch 36/300\n",
      "118633/118633 [==============================] - 105s 889us/step - loss: 1.7903 - acc: 0.5676\n",
      "Epoch 37/300\n",
      "118633/118633 [==============================] - 105s 888us/step - loss: 1.8609 - acc: 0.5564\n",
      "Epoch 38/300\n",
      "118633/118633 [==============================] - 106s 890us/step - loss: 1.8176 - acc: 0.5630\n",
      "Epoch 39/300\n",
      "118633/118633 [==============================] - 105s 889us/step - loss: 1.7714 - acc: 0.5722\n",
      "Epoch 40/300\n",
      "118633/118633 [==============================] - 106s 890us/step - loss: 1.7659 - acc: 0.5721\n",
      "Epoch 41/300\n",
      "118633/118633 [==============================] - 106s 889us/step - loss: 1.7622 - acc: 0.5724\n",
      "Epoch 42/300\n",
      "118633/118633 [==============================] - 104s 878us/step - loss: 1.7832 - acc: 0.5716\n",
      "Epoch 43/300\n",
      "118633/118633 [==============================] - 107s 900us/step - loss: 1.7651 - acc: 0.5742\n",
      "Epoch 44/300\n",
      "118633/118633 [==============================] - 102s 857us/step - loss: 1.7726 - acc: 0.5737\n",
      "Epoch 45/300\n",
      "118633/118633 [==============================] - 105s 881us/step - loss: 1.7703 - acc: 0.5758\n",
      "Epoch 46/300\n",
      "118633/118633 [==============================] - 102s 859us/step - loss: 1.7982 - acc: 0.5674\n",
      "Epoch 47/300\n",
      "118633/118633 [==============================] - 117s 985us/step - loss: 1.7241 - acc: 0.5843\n",
      "Epoch 48/300\n",
      "118633/118633 [==============================] - 113s 952us/step - loss: 1.7437 - acc: 0.5778\n",
      "Epoch 49/300\n",
      "118633/118633 [==============================] - 102s 861us/step - loss: 1.7031 - acc: 0.5853\n",
      "Epoch 50/300\n",
      "118633/118633 [==============================] - 102s 860us/step - loss: 1.6602 - acc: 0.5949\n",
      "Epoch 51/300\n",
      "118633/118633 [==============================] - 102s 864us/step - loss: 1.7308 - acc: 0.5743\n",
      "Epoch 52/300\n",
      "118633/118633 [==============================] - 103s 866us/step - loss: 1.6441 - acc: 0.5955\n",
      "Epoch 53/300\n",
      "118633/118633 [==============================] - 106s 895us/step - loss: 1.5960 - acc: 0.6072\n",
      "Epoch 54/300\n",
      "118633/118633 [==============================] - 106s 890us/step - loss: 1.5844 - acc: 0.6104\n",
      "Epoch 55/300\n",
      "118633/118633 [==============================] - 106s 892us/step - loss: 1.5952 - acc: 0.6053\n",
      "Epoch 56/300\n",
      "118633/118633 [==============================] - 106s 891us/step - loss: 1.6030 - acc: 0.6023\n",
      "Epoch 57/300\n",
      "118633/118633 [==============================] - 106s 890us/step - loss: 1.5675 - acc: 0.6117\n",
      "Epoch 58/300\n",
      "118633/118633 [==============================] - 106s 891us/step - loss: 1.5541 - acc: 0.6148\n",
      "Epoch 59/300\n",
      "118633/118633 [==============================] - 106s 893us/step - loss: 1.5535 - acc: 0.6145\n",
      "Epoch 60/300\n",
      "118633/118633 [==============================] - 105s 884us/step - loss: 1.5666 - acc: 0.6100\n",
      "Epoch 61/300\n",
      "118633/118633 [==============================] - 109s 923us/step - loss: 1.5328 - acc: 0.6178\n",
      "Epoch 62/300\n",
      "118633/118633 [==============================] - 99s 838us/step - loss: 1.5105 - acc: 0.6245\n",
      "Epoch 63/300\n",
      "118633/118633 [==============================] - 102s 857us/step - loss: 1.5055 - acc: 0.6240\n",
      "Epoch 64/300\n",
      "118633/118633 [==============================] - 102s 862us/step - loss: 1.4928 - acc: 0.6259\n",
      "Epoch 65/300\n",
      "118633/118633 [==============================] - 103s 865us/step - loss: 1.4951 - acc: 0.6252\n",
      "Epoch 66/300\n",
      "118633/118633 [==============================] - 102s 863us/step - loss: 1.4941 - acc: 0.6246\n",
      "Epoch 67/300\n",
      "118633/118633 [==============================] - 106s 896us/step - loss: 1.5069 - acc: 0.6237\n",
      "Epoch 68/300\n",
      "118633/118633 [==============================] - 107s 904us/step - loss: 1.4649 - acc: 0.6332\n",
      "Epoch 69/300\n",
      "118633/118633 [==============================] - 107s 905us/step - loss: 1.4976 - acc: 0.6236\n",
      "Epoch 70/300\n",
      "118633/118633 [==============================] - 106s 896us/step - loss: 1.4529 - acc: 0.6364\n",
      "Epoch 71/300\n",
      "118633/118633 [==============================] - 106s 894us/step - loss: 1.4176 - acc: 0.6445\n",
      "Epoch 72/300\n",
      "118633/118633 [==============================] - 105s 881us/step - loss: 1.4095 - acc: 0.6452\n",
      "Epoch 73/300\n",
      "118633/118633 [==============================] - 108s 911us/step - loss: 1.4155 - acc: 0.6447\n",
      "Epoch 74/300\n",
      "118633/118633 [==============================] - 107s 899us/step - loss: 1.4113 - acc: 0.6447\n",
      "Epoch 75/300\n",
      "118633/118633 [==============================] - 107s 906us/step - loss: 1.4105 - acc: 0.6444\n",
      "Epoch 76/300\n",
      "118633/118633 [==============================] - 106s 894us/step - loss: 1.4549 - acc: 0.6340\n",
      "Epoch 77/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118633/118633 [==============================] - 137s 1ms/step - loss: 1.4164 - acc: 0.6446\n",
      "Epoch 78/300\n",
      "118633/118633 [==============================] - 105s 883us/step - loss: 1.6406 - acc: 0.5983\n",
      "Epoch 79/300\n",
      "118633/118633 [==============================] - 104s 874us/step - loss: 1.5372 - acc: 0.6250\n",
      "Epoch 80/300\n",
      "118633/118633 [==============================] - 103s 867us/step - loss: 1.4784 - acc: 0.6394\n",
      "Epoch 81/300\n",
      "118633/118633 [==============================] - 130s 1ms/step - loss: 1.4493 - acc: 0.6450\n",
      "Epoch 82/300\n",
      "118633/118633 [==============================] - 108s 914us/step - loss: 1.4748 - acc: 0.6389\n",
      "Epoch 83/300\n",
      "118633/118633 [==============================] - 110s 931us/step - loss: 1.4681 - acc: 0.6381\n",
      "Epoch 84/300\n",
      "118633/118633 [==============================] - 143s 1ms/step - loss: 1.4597 - acc: 0.6396\n",
      "Epoch 85/300\n",
      "118633/118633 [==============================] - 107s 902us/step - loss: 1.4472 - acc: 0.6443\n",
      "Epoch 86/300\n",
      "118633/118633 [==============================] - 108s 909us/step - loss: 1.4321 - acc: 0.6463\n",
      "Epoch 87/300\n",
      "118633/118633 [==============================] - 113s 951us/step - loss: 1.4305 - acc: 0.6452\n",
      "Epoch 88/300\n",
      "118633/118633 [==============================] - 144s 1ms/step - loss: 1.4213 - acc: 0.6465\n",
      "Epoch 89/300\n",
      "118633/118633 [==============================] - 113s 955us/step - loss: 1.4335 - acc: 0.6431\n",
      "Epoch 90/300\n",
      "118633/118633 [==============================] - 146s 1ms/step - loss: 1.4552 - acc: 0.6388\n",
      "Epoch 91/300\n",
      "118633/118633 [==============================] - 110s 927us/step - loss: 1.3892 - acc: 0.6566\n",
      "Epoch 92/300\n",
      "118633/118633 [==============================] - 107s 901us/step - loss: 1.3766 - acc: 0.6583\n",
      "Epoch 93/300\n",
      "118633/118633 [==============================] - 150s 1ms/step - loss: 1.3911 - acc: 0.6540\n",
      "Epoch 94/300\n",
      "118633/118633 [==============================] - 110s 929us/step - loss: 1.3979 - acc: 0.6506\n",
      "Epoch 95/300\n",
      "118633/118633 [==============================] - 107s 899us/step - loss: 1.4602 - acc: 0.6321\n",
      "Epoch 96/300\n",
      "118633/118633 [==============================] - 150s 1ms/step - loss: 1.3201 - acc: 0.6665\n",
      "Epoch 97/300\n",
      "118633/118633 [==============================] - 109s 920us/step - loss: 1.3194 - acc: 0.6636\n",
      "Epoch 98/300\n",
      "118633/118633 [==============================] - 105s 883us/step - loss: 1.2810 - acc: 0.6763\n",
      "Epoch 99/300\n",
      "118633/118633 [==============================] - 103s 865us/step - loss: 1.2834 - acc: 0.6749\n",
      "Epoch 100/300\n",
      "118633/118633 [==============================] - 107s 906us/step - loss: 1.3377 - acc: 0.6618\n",
      "Epoch 101/300\n",
      "118633/118633 [==============================] - 105s 885us/step - loss: 1.3051 - acc: 0.6702\n",
      "Epoch 102/300\n",
      "118633/118633 [==============================] - 110s 925us/step - loss: 1.3069 - acc: 0.6696\n",
      "Epoch 103/300\n",
      "118633/118633 [==============================] - 153s 1ms/step - loss: 1.3227 - acc: 0.6674\n",
      "Epoch 104/300\n",
      "118633/118633 [==============================] - 113s 950us/step - loss: 1.3277 - acc: 0.6655\n",
      "Epoch 105/300\n",
      "118633/118633 [==============================] - 110s 929us/step - loss: 1.3659 - acc: 0.6591\n",
      "Epoch 106/300\n",
      "118633/118633 [==============================] - 113s 954us/step - loss: 1.3519 - acc: 0.6642\n",
      "Epoch 107/300\n",
      "118633/118633 [==============================] - 147s 1ms/step - loss: 1.3723 - acc: 0.6614\n",
      "Epoch 108/300\n",
      "118633/118633 [==============================] - 114s 962us/step - loss: 1.3560 - acc: 0.6636\n",
      "Epoch 109/300\n",
      "118633/118633 [==============================] - 112s 945us/step - loss: 1.5385 - acc: 0.6266\n",
      "Epoch 110/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 1.5495 - acc: 0.6283\n",
      "Epoch 111/300\n",
      "118633/118633 [==============================] - 104s 878us/step - loss: 1.3196 - acc: 0.6730\n",
      "Epoch 112/300\n",
      "118633/118633 [==============================] - 152s 1ms/step - loss: 1.3313 - acc: 0.6721\n",
      "Epoch 113/300\n",
      "118633/118633 [==============================] - 112s 944us/step - loss: 1.3468 - acc: 0.6675\n",
      "Epoch 114/300\n",
      "118633/118633 [==============================] - 102s 857us/step - loss: 1.2989 - acc: 0.6743\n",
      "Epoch 115/300\n",
      "118633/118633 [==============================] - 103s 870us/step - loss: 1.2909 - acc: 0.6764\n",
      "Epoch 116/300\n",
      "118633/118633 [==============================] - 103s 870us/step - loss: 1.2853 - acc: 0.6758\n",
      "Epoch 117/300\n",
      "118633/118633 [==============================] - 101s 853us/step - loss: 1.4174 - acc: 0.6500\n",
      "Epoch 118/300\n",
      "118633/118633 [==============================] - 102s 860us/step - loss: 1.3080 - acc: 0.6749\n",
      "Epoch 119/300\n",
      "118633/118633 [==============================] - 102s 864us/step - loss: 1.2846 - acc: 0.6770\n",
      "Epoch 120/300\n",
      "118633/118633 [==============================] - 103s 865us/step - loss: 1.2856 - acc: 0.6775\n",
      "Epoch 121/300\n",
      "118633/118633 [==============================] - 104s 873us/step - loss: 1.2698 - acc: 0.6807\n",
      "Epoch 122/300\n",
      "118633/118633 [==============================] - 114s 963us/step - loss: 1.3276 - acc: 0.6690\n",
      "Epoch 123/300\n",
      "118633/118633 [==============================] - 134s 1ms/step - loss: 1.3723 - acc: 0.6659\n",
      "Epoch 124/300\n",
      "118633/118633 [==============================] - 110s 931us/step - loss: 1.5784 - acc: 0.6353\n",
      "Epoch 125/300\n",
      "118633/118633 [==============================] - 151s 1ms/step - loss: 1.5216 - acc: 0.6426\n",
      "Epoch 126/300\n",
      "118633/118633 [==============================] - 114s 958us/step - loss: 1.4222 - acc: 0.6603\n",
      "Epoch 127/300\n",
      "118633/118633 [==============================] - 153s 1ms/step - loss: 1.3910 - acc: 0.6655\n",
      "Epoch 128/300\n",
      "118633/118633 [==============================] - 113s 952us/step - loss: 1.4151 - acc: 0.6573\n",
      "Epoch 129/300\n",
      "118633/118633 [==============================] - 109s 915us/step - loss: 1.4183 - acc: 0.6617\n",
      "Epoch 130/300\n",
      "118633/118633 [==============================] - 148s 1ms/step - loss: 1.3859 - acc: 0.6646\n",
      "Epoch 131/300\n",
      "118633/118633 [==============================] - 114s 961us/step - loss: 1.3054 - acc: 0.6706\n",
      "Epoch 132/300\n",
      "118633/118633 [==============================] - 145s 1ms/step - loss: 1.2291 - acc: 0.6895\n",
      "Epoch 133/300\n",
      "118633/118633 [==============================] - 114s 964us/step - loss: 1.2132 - acc: 0.6927\n",
      "Epoch 134/300\n",
      "118633/118633 [==============================] - 146s 1ms/step - loss: 1.2246 - acc: 0.6886\n",
      "Epoch 135/300\n",
      "118633/118633 [==============================] - 112s 947us/step - loss: 1.2351 - acc: 0.6856\n",
      "Epoch 136/300\n",
      "118633/118633 [==============================] - 150s 1ms/step - loss: 1.2568 - acc: 0.6823\n",
      "Epoch 137/300\n",
      "118633/118633 [==============================] - 114s 959us/step - loss: 1.2122 - acc: 0.6936\n",
      "Epoch 138/300\n",
      "118633/118633 [==============================] - 107s 904us/step - loss: 1.2114 - acc: 0.6936\n",
      "Epoch 139/300\n",
      "118633/118633 [==============================] - 151s 1ms/step - loss: 1.2477 - acc: 0.6882\n",
      "Epoch 140/300\n",
      "118633/118633 [==============================] - 114s 960us/step - loss: 1.2322 - acc: 0.6886\n",
      "Epoch 141/300\n",
      "118633/118633 [==============================] - 114s 962us/step - loss: 1.3209 - acc: 0.6768\n",
      "Epoch 142/300\n",
      "118633/118633 [==============================] - 148s 1ms/step - loss: 1.2964 - acc: 0.6827\n",
      "Epoch 143/300\n",
      "118633/118633 [==============================] - 119s 1ms/step - loss: 1.3667 - acc: 0.6747\n",
      "Epoch 144/300\n",
      "118633/118633 [==============================] - 111s 934us/step - loss: 1.3618 - acc: 0.6724\n",
      "Epoch 145/300\n",
      "118633/118633 [==============================] - 154s 1ms/step - loss: 2.2683 - acc: 0.5391\n",
      "Epoch 146/300\n",
      "118633/118633 [==============================] - 114s 963us/step - loss: 2.3158 - acc: 0.5337\n",
      "Epoch 147/300\n",
      "118633/118633 [==============================] - 110s 929us/step - loss: 2.3334 - acc: 0.5225\n",
      "Epoch 148/300\n",
      "118633/118633 [==============================] - 148s 1ms/step - loss: 2.0448 - acc: 0.5592\n",
      "Epoch 149/300\n",
      "118633/118633 [==============================] - 115s 972us/step - loss: 1.7197 - acc: 0.6118\n",
      "Epoch 150/300\n",
      "118633/118633 [==============================] - 149s 1ms/step - loss: 1.7000 - acc: 0.6119\n",
      "Epoch 151/300\n",
      "118633/118633 [==============================] - 117s 983us/step - loss: 1.5592 - acc: 0.6387\n",
      "Epoch 152/300\n",
      "118633/118633 [==============================] - 144s 1ms/step - loss: 1.7290 - acc: 0.6016\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118633/118633 [==============================] - 109s 921us/step - loss: 1.4157 - acc: 0.6568\n",
      "Epoch 154/300\n",
      "118633/118633 [==============================] - 102s 858us/step - loss: 1.3733 - acc: 0.6604\n",
      "Epoch 155/300\n",
      "118633/118633 [==============================] - 104s 880us/step - loss: 1.2918 - acc: 0.6824\n",
      "Epoch 156/300\n",
      "118633/118633 [==============================] - 110s 925us/step - loss: 1.4235 - acc: 0.6611\n",
      "Epoch 157/300\n",
      "118633/118633 [==============================] - 111s 932us/step - loss: 1.5006 - acc: 0.6496\n",
      "Epoch 158/300\n",
      "118633/118633 [==============================] - 107s 900us/step - loss: 1.5210 - acc: 0.6476\n",
      "Epoch 159/300\n",
      "118633/118633 [==============================] - 107s 899us/step - loss: 2.0624 - acc: 0.5568\n",
      "Epoch 160/300\n",
      "118633/118633 [==============================] - 107s 901us/step - loss: 1.9931 - acc: 0.5406\n",
      "Epoch 161/300\n",
      "118633/118633 [==============================] - 107s 904us/step - loss: 2.0166 - acc: 0.5442\n",
      "Epoch 162/300\n",
      "118633/118633 [==============================] - 107s 901us/step - loss: 1.7972 - acc: 0.5959\n",
      "Epoch 163/300\n",
      "118633/118633 [==============================] - 107s 898us/step - loss: 1.6909 - acc: 0.6113\n",
      "Epoch 164/300\n",
      "118633/118633 [==============================] - 107s 898us/step - loss: 1.6163 - acc: 0.6314\n",
      "Epoch 165/300\n",
      "118633/118633 [==============================] - 107s 901us/step - loss: 1.5871 - acc: 0.6371\n",
      "Epoch 166/300\n",
      "118633/118633 [==============================] - 107s 902us/step - loss: 1.5570 - acc: 0.6438\n",
      "Epoch 167/300\n",
      " 61568/118633 [==============>...............] - ETA: 53s - loss: 1.4661 - acc: 0.6636"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9ee23d5d1385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=300)\n",
    "model.save('model.h5')\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = load_doc('TheRepublic_Sequences.txt')\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split())-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education furnish the best safeguard but they are welleducated already he replied i cannot be so confident my dear glaucon i said i am much more certain that they ought to be and that true education whatever that may be will have the greatest tendency to civilize and humanize them in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = lines[randint(0, len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        #encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        #truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        \n",
    "        #predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "                \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the metaphor and have to be called rarely things and the busybodies is defined down into the hands of the man and the neglect of the gods and the gods are bowed with fruit and the bed and the gods and not real however is nobody else fears and constituted\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
